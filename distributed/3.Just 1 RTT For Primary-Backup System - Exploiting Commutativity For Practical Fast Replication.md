# Just 1 RTT For Primary-Backup System - Exploiting Commutativity For Practical Fast Replication

(本文属于原创内容，首发于https://github.com/dragon-distributed/book , 未经作者(龙永超)许可，不得转载。)  

## 背景

这篇paper全称是Exploiting Commutativity For Practical Fast Replication，发布在USENIX，在2019年发布的，可以直接在google上找到相应的Paper。

Paper提出一种针对passive replication分布式系统，1 RTT的解决方案。  

Passive Replication，也叫Primary-Backup，是一种非常常见的复制方案，基于Raft/Paxos/ZAB一致性算法的系统，都是使用这种复制模式的，如ZK，ETCD等等。MySQL也是基于Primary-Backup的复制模式。

传统的Passive Replication是这个样子的：  
![Passive replication](https://longdandan-1256672193.cos.ap-guangzhou.myqcloud.com/article/distributed/1.passive-replication.jpg)

非常容易看出，一次请求，最少需要2个RTT才能处理完，一个RTT是Client<->Primary，另一个是Primary<->Backup。  

此Paper提出了一种名为Consistent Unordered Replication Protocol (CURP)的方案，将2RTT->1RTT。这种方案，将Latency减少了一半。  

  
## 思路

简单来说，CURP结合了"Fan-Out"、"Async Replication"以及"reply storage"的思路，如下图所示：  

![overall](https://longdandan-1256672193.cos.ap-guangzhou.myqcloud.com/article/distributed/3.overall.jpg)

Witness是CURP引入的，用于存储操作的一个组件。正常情况下，Client同时往Witness以及Primary写入数据，Primary只要本地完成写入，即可Ack Client，异步同步到Backups。Client在收到Primary以及Witness的Ack后，则认为操作成功。根据此思路，确实只需要1 RTT即可完成操作，此RTT为MAX(RTT, Client->Primary, Client->Witness)。    

在了解完思路后，本文先讨论一些关键细节，然后把正常写入流程以及异常流程总结出来。    

## 关键细节

### Witness要多少个？

f+1个，根据实际情况，如果工程上，可以容忍f个witness crash，则使用f+1个witness即可。  

### Witness写入是Majority？Full？还是Single？

其实Witness也是一个一致性集群，如果这个集群跑类似于Raft或者其它，显然是不合适的，因为如果跑Raft/Paxos，也是2RTT。如果是写一个witness，然后异常复制到其它witness，则显示不能满足durability的要求。所以CURP走的是唯一一条路子：Full Write，也就是Client要写入所有Witness。  

写入所有的Witness，则肯定需要处理部分写入的情况，在讨论部分写入的情况之前，我们先看看Witness的约束。  

### Witness的约束

Witness存储的Request是无序的，在Primary crash的情况下（下文中的Primary都是指数据集群的节点，非Witness），需要从Witness中回放数据。所以，这要求Witness中的Request是排它的，在Paper中称为"commutative"。  

在Key-Value系统中，这个commutative比较好辨认，举个例子，如果Witness中有存 key a = 10，然后Witness如果再收到Request: key a = 20，则Witness需要拒绝此请求，这是因为无序的原因，如果最后需要回放到Primary，则不能确认Request的顺序。在拒绝此请求后，Client需走异常流程。  

因为Primary与Witness分布于不同的地方，所以如果在Primary crash，新Primary需要在Witness回放时，有可能会重复执行某一个Request。为了在这种情况下，保证一致性，个人认为有两个方式：  
一种是Paper提出的，使用RIFL中的exactly-once语义，即为每个Request添加Unique ID。  
另一种是个人想到的，使用幂等的方式，举个例子，Witness中存储的Request必须是 set key a = 10这种类型，而不是set key a = a + 10这种类型的。  


### Witness写入部分失败怎么办？

Full Write肯定需要处理部分写成功，部分失败的情况。失败可能是由于Witness异常(crash/hang等)，或者是逻辑异常，如上述的不满足commutative的情况。  

在Client得到Witness拒绝响应，或者在一定时间内，部分Witness无响应的情况下，Client需要发送Sync RPC到Primary，查询此Request的状态，Primary在收到Sync RPC后，只有在确认此Request成功复制到Backup后，才会Ack。  

可以理解为，Client觉得Witness不可依靠了，还是依靠原集群的复制协议，所以Client会去Primary去查询这个Request是否成功了（如Paxos是Majority），当Primary回复成功，则此Request才能算是成功。  

在异常流程下，Latency由1 RTT会扩大到 2-3个RTT，2个RTT是指，在Sync RPC发送时，Primary已经复制成功了，如下图：  

![图2](https://longdandan-1256672193.cos.ap-guangzhou.myqcloud.com/article/distributed/3.2rtt.jpg)

3个RTT则是，在Sync RPC发送时，Primary还没有复制，所以Sync RPC需要2 RTT，加上之前的Request RPC，则共需3 RTT，如下图：  

![图3](https://longdandan-1256672193.cos.ap-guangzhou.myqcloud.com/article/distributed/3.3rtt.jpg)

### Primary的约束

Witness有约束，Primary也有自己的约束，也就是Primary如果发现新的Request与现有的，未达成多数派的（未提交）的Request有排它性，如Key-Value系统有相同的key，则Primary需要达成多数派后，才能ack client。  

为什么需要这样，paper举了个例子，如client要写x=2的操作，这时，primary发现有key=x的其它request还没提交，如果这时在primary执行并ack client，而有另一个client读会取到x=2的值。但这时如果primary crash，而witness极有可能没有这个request，所以这时，读client读到了一个错误的值，违反了一致性。  

### Witness的Garbage Collection

Witness不能无限存储数据，所以需要GC，而且Witness因为排它(commutativity)性，GC能有效地减少Witness存储的数据从而减少Rejection的概率。  

GC是由Primary往Witness发起的，Primary在完成Backups的同步之后，往Witness发送GC RPC，告诉Witness，哪些Request可以删除了。Witness收到GC Request之后，删除相应的数据，并回复Primary，这个回复会带上那些存在Witness很久，但一直没被删除的Request，让Primary在下一次触发GC时，告诉Witness这些Request的状态。  

### Witness的Reconfigurations

#### 流程
在Witness异常，或者有计划地对Witness进行维护，就需要Witness的Reconfiguration。  

在Paper中，Witness的集群状态是由一个叫"system configuration manager"维护的，让它决定剔除哪些Witness，添加哪些Witness。在system configuration manager作了决定后，会通知Primary，Primary会将新的Witness集群信息存起来，并同步到Backups，在同步完成后，ack system configuration manager，新的集群正式生效，画个图看一下：  

![图4](https://longdandan-1256672193.cos.ap-guangzhou.myqcloud.com/article/distributed/3.reconfiguration.jpg)

在实际中，system configuration manager可以是一个独立的组件，也可以由Primary兼任。  

下面讨论一下关键点

#### Client如何感知Witness集群的变化

Witness是不会主动通知Client中，应该是Client主动发现。  

CURP用了比较常见的方式，就是版本号的方式。对于所有写入请求，Client会带上当前Witness集群信息的Version，如果Primary发现Version不对，则拒绝请求，并告诉Client最新的Witness集群信息。而Witness集群信息更新之后，Primary的Version会相应变化。 
 
#### Witness在变更过程中，Client的写入流程

在工程实现中，可以用最简单粗暴的办法，在Witness更新的过程吧，Client走异常流程。  

### Backup读

传统的Primary-Backup系统，一般会在Primary读，提供Linearizability(强一致性)。但现在越来越多系统会开发Backup读，有一些系统提供了backup的最终一致性读，有一些则提供了线性一致性读，通过位点等方式实现。  

Paper介绍了CURP如何在Backup上进行线性一致性读取。如下图：  

![图5](https://longdandan-1256672193.cos.ap-guangzhou.myqcloud.com/article/distributed/3.backup%20read.jpg)

简单来说，就是去Witness查查是否有需要读的key，如果有，则必须去Primary中读，如果没有，可以直接从Backup上读。这个比较好理解，就不展开了。  


## 流程总结

### 正常流程

Normal cases：
1) client同时往所有的Witness以及Primary写入 
2) Primary检查commutative，若不排它，则采用异常同步到backup的方式进行   
2) client等待所有的Witness的ack以及Primary的ack  

### 异常流程

1) client同时往所有的Witness以及Primary写入
2) primary检查commutative，若排它，则需要达成共识才ack  
3) Witness若reject client，或者Witness不回client，client需要发送Sync RPC到Primary查询状态，整个流程需要2-3RTT  