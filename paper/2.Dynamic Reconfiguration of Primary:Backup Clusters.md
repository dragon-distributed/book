# Dynamic Reconfiguration of Primary/Backup Clusters

(本文属于原创内容，首发于https://github.com/dragon-distributed/book , 未经作者许可，不得转载。)  

## 背景

这篇paper发布在USENIX ATC 2012，可以在https://www.researchgate.net/publication/255564354_Dynamic_Reconfiguration_of_PrimaryBackup_Clusters 中找到。

## 写在前面

在快速通读了全文后，觉得思路与Raft协议中的Membership changes中的joint consensus非常相似，一直在思考为什么又再发一篇paper介绍，最后想到发表时间，\<\<In Search of an Understandable Consensus Algorithm\>\>是在2013年发布的，而这篇是2012年发布的，这样就很明白了。
(PS: 再翻看Diego Ongaro博士论文，里面有ZAB的描述验证了我的思想"Zab’s approach to membership changes is the closest to Raft’s joint consensus approach, and the basic idea would also work for Raft.")

Raft有两种方式做Membership changes，一种是raft paper中的joint consensus，可以一次性变更多个节点，可以多个变更同时进行。另一种是"one-by-one", 在raft作者的博士论文中提出，意思是每次只能操作一个节点的更新，在上次变更完成之前，不允许再次变更，etcd用的就是这种实现。对于这种实现，可以满足一般的工程需求（使用需求），也可以避免在实现时的复杂性。 

## 关键思想

Primary/Backup replication(under primary order) + RSM(replication state-machine) + majority，意思是，利用复制，把config update log entry复制在多数派节点，并达成一致。

## High level description

论文中有high level的说明，大体上是这样的：

![generic](https://longdandan-1256672193.cos.ap-guangzhou.myqcloud.com/article/paper/2.generic.jpg)

假设S为旧配置，S'为新配置，步骤为：
1) 复制"新配置"，并达到多数派（S），然后持久化保存S'配置
2) 停用旧配置S，使得没有后续的请求是在S的多数派中达成的。
3) 使新节点追数据，并使集群S'能形成多数派（比如上述3个点节，添加2个，则最少需要3个节点是最新的数据）
4) 激活（使用）新配置S'，对外提供服务。

high level description可能有许多细节不太清楚，我们接下来看看细一些的。

## Stable primary流程

![Stable primary](https://longdandan-1256672193.cos.ap-guangzhou.myqcloud.com/article/paper/2.stable_primary_reconfiguration.jpg)  

论文中分为以下几步：
1) **pre-step：**先让S'的新节点追数据，目的是为了减少变更为S'时停服时间  
2) **step 1：** 在log entry的尾端，加上reconfiguration的信息（下文称这个消息为cop），并复制到已连接的backups(and/or to S')  
3) **step 2：** Primary在发送变更之后，不需要停服，而是将把在cop后的请求，用S'处理（多数派的判断使用S'等）  
4) **step 3：** Primary等待cop在S'达成多数派  
5) **step 4：** 一旦cop在S与S'达成多数派后，Primary将发送Activate消息去commit cop，并active S'。


## Reconfiguring the Clients

论文中指出一种client connection rebalance的算法，用于在membership changes之后，平衡每个节点(in S')的客户端连接。

比较学术，这里记录下来，待需时，再仔细研读。

![Reconfiguring the Clients](https://longdandan-1256672193.cos.ap-guangzhou.myqcloud.com/article/paper/2.reconfigure%20client%20connection.jpg)

## 与Raft Membership Changes的比较

直接摘抄:  

Zab’s approach to membership changes is the closest to Raft’s joint consensus approach, and the basic idea would also work for Raft. The two phases in Zab’s approach are:  1) A log entry containing the new configuration is committed to both a majority of the old cluster and a majority of the new cluster. The old leader may continue to replicate entries past the new configuration entry, but it may not mark any further entries committed (unless it is also part of the new cluster).  2) Then, the leader of the old cluster sends Activate messages to the new cluster, informing the new cluster of the configuration entry’s commitment. This enables the new cluster to elect a leader and continue operations. If the old leader is also part of the new cluster, it can continue as leader.    

Raft’s joint consensus approach records state during membership changes more explicitly in the log: it uses a second log entry to activate the new configuration, whereas Zab uses Activate messages that are not logged. This makes Zab’s transitions and failure recovery more complex, as a server’s current configuration depends on both its log and its latest committed configuration. In Raft, on the other hand, a server always uses the latest configuration in its log, and failures are handled with no additional mechanism.  
Neither algorithm stalls client operations when the old leader is also part of the new cluster, as this server continues as leader throughout and beyond the membership change. However, Zab’s treatment of leaders that are being removed from the cluster differs from Raft’s in two ways:  1) In Raft a leader that is being removed continues to commit log entries until it steps down. In Zab, however, a leader that is being removed may not commit any log entries that come after the configuration change entry in its log. It may still replicate those entries, though, and the effect of this restriction is probably small.  
2) In Zab if the leader removes itself from the cluster, the new servers will begin leader election right away, and the old leader can designate a new server to become leader immediately. In Raft the new servers wait for an election timeout, but using Raft’s leadership transfer extension (Chapter 3) can similarly avoid this delay.  

ZooKeeper allows reads to be served by any server, and, without additional mechanism, clients may end up imbalanced across servers after membership changes. For example, servers that have recently been added to a cluster will have a disproportionately low number of clients connected to them. The paper describes a probabilistic algorithm to rebalance client load to the new servers after a membership change, which would also be useful for Raft implementations that allow reads from any server.