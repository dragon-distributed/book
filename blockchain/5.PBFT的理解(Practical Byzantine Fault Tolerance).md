# PBFT的理解 (Practical Byzantine Fault Tolerance)
(本文属于原创内容，首发于https://github.com/dragon-distributed/book , 未经作者许可，不得转载。)  

## 背景

分布式系统从小世界（能自己全盘控制）走到大世界（广域网的世界），所面临的挑战是完全不同的。  

小世界里面的人(node)，都是善良的，他们可能会不回消息，或者延时回你消息，但是他不会说慌，但大世界就不一样了，充满着形形色色的人，他们可能会回你ok，但自己做不干事（说一套做一套），或者他可能跟你说某事件达成共识是甲，转头又跟别人说乙（墙头草），更恶劣的是专门干坏事的，比如欺骗你。  

在小世界里，使用如paxos、raft、zab就可以让一群人(cluster)达成共识，但在大世界里，就需要如Byzantine Raft，PBFT等能容忍拜占庭的一致性协议，才能让一群人达成共识。

PBFT的论文是在1999年发表的，近20年的时间都没有出名，也没有被广范应用，除了计算机技术发展外，个人认为PBFT有他自己的"落地悖论"：如果集群大，则PBFT会受性能问题的制约，但如果集群小，又可以把他控制在小世界的范围内，可选的算法太多，没必要使用它。随着区块链的发展，Algorand中提出使用"选子集"跑BFT的思想，越来越多系统考虑使用PBFT作为区块链的共识算法，PBFT也走向了大家的视线。

由于笔者是搞分布式的，所以本文考虑从Raft的角度出发，聊聊PBFT。

## PBFT算法组成

PBFT主要由三个流程组成，分别是：

1) 共识流程：主要是指从接收到client request，到复制，到达成共识，ack client的流程，可以看成Raft中Append Entries。
2) View Change：主要是指Primary异常时，换Primary的过程，可以看成是Raft的Leader Change，View可以看作是Term。
3) Garbage Collection（GC）：可以看作是节点做snapshot，但与raft不同，他需要多节点交互的，后面会细说。

再说流程前，想先介绍一下pbft的一些关键点。因为流程可能比较多文章有介绍，但一些关键点我想先表达出来，以防止文章太长，后面没心思把重点整出来。

## 为什么是集群节点数n=3f+1，prepare与commit是2f+1，client是f+1

### 为什么是集群节点数n=3f+1

首先我们先来了解raft/paxos这种非Byzantine算法，为什么是n=2f+1。在非Byzantine的场景下，假设有n个节点，突然倒下了f个，那么只要剩下的节点大于f+1个(多数派)，则可以达成共识，所以n>2f，推导出n>=2f+1，按最少成本算，n=2f+1。举个例子，比如有5个节点，倒了2个，只要有>2的节点(3个)存在，就能达成多数派，如果倒了3个，剩下2个，则达不到多数派。  

但在Byzantine的场景下，倒下的可能不是作恶的节点。在极端的情况下，倒下了f个，作恶节点有f个，那只要剩下的节点大于f+1个(这些都是诚实的节点)，则可以达成多数派，达成共识，所以n>3f，推导出n>=3f+1，n=3f+1。这个理解起来有点费神，或者把结论n=3f+1记着也是不错的选择。

paper原文是这样子的：
"The resiliency of our algorithm is optimal: 3f+1 is the minimum number of replicas that allow an asynchronous system to provide the safety and liveness properties when up to f replicas are faulty (see [2] for a proof). This many replicas are needed because it must be possible to proceed after communicating with n-f replicas, since
replicas might be faulty and not responding. However, it is possible that the f replicas that did not respond are not faulty and, therefore, f of those that responded might be faulty. Even so, there must still be enough responses that those from non-faulty replicas outnumber those from faulty ones, i.e., n-2f>f . Therefore n>3f."

### 为什么prepare与commit是2f+1

我们按节点A的视角看，如果他是作恶节点，那么prepare与commit他想怎么弄就怎么弄，我们不管，如果他是正常节点，假设除A之外，有f个倒下节点，f个作恶节点。那么，对于倒下的节点，他是收不到消息的，他可能收到f个作恶节点的消息，那么，只要他再收比作恶节点多的消息，就可以达成多数派。也就是收到 f(作恶节点个数) + (f + 1)(比作恶节点多的个数) = 2f+1，这包括自己。

举个例子，假设f=1，则集群有4个节点，节点A收到 ([f=1]作恶节点个数) + (f + 1) (比作恶节点多的个数)= 3个节点时，则可以达到多数派。

### 为什么client是f+1

对于client来说，收到f+1的ack，极端情况下，这个收到的f个ack都是作恶节点的，那剩下的1肯定是诚实节点的，那么这个节点保证了这个请求在多数派达成了一致(已commit)，也保证了正确性。

## 共识流程

(PS:本人认为用代码实现一次跟看N遍论文的效果是完全不同，本人计划后面有时间会用代码demo一遍，所以这里的太细节元素就略去了，待实现时再完全补全技术细节)

Normal case的共识流程主要有五个步骤，paper写得比较清楚，如下图：

![图1](https://longdandan-1256672193.cos.ap-guangzhou.myqcloud.com/article/blockchain/5.pbftprocess.jpg)

a) **client发送请求到primary**。  
b) **pre-prepare:**一旦primary收到request，它会做三件事：  
   1) 给request定序(the primary assigns a sequence number, n , to the request)  
   2) 广播pre-prepare(包括view number(类似raft term), sequence number(类似raft index), request, 签名等信息)  
   3) 将pre-prepare message存储到本地的log  
c) **prepare:** 其它backups收到pre-prepare message之后，会进行校验。如果校验通过，则将prepare message存储到本地的log  
d) **commit:** 当backups收到包括自己的2f+1个有效的prepare消息之后，它开始commit阶段的流程，广播commit消息。其它节点收到commit消息后，进行校验，随后存储到本地的log。（本人很好奇为什么paper没写广播的节点储存不储存commit消息，但个人认为是要存的）  
e) reply：当backup收到包括自己的2f+1个有效的commit消息后，执行request，并reply给client，这跟raft的apply是一样的   

这里的每一步都要存储log，在工程实现中，需要考虑采用什么存储方式，可以减少disk io操作，一来提升性能，二来省存储空间。

## Garbage Collection（GC）

跟raft的snapshot一样，总不能让log不断增长，要生成一个snapshot，包括状态机的一个dump跟一个log的位点，然后删除这个位点前的log。  

在raft算法中，由于没有Byzantine，所以节点可以自己生成snapshot，其它节点在恢复时，认为只要是snapshot都是有效的（诚实的）。但PBFT不一样，节点在用snapshot时，必须有办法证明此snapshot是有效的（诚实的），所以PBFT在生成snapshot时，必须有proof。  

看了几遍paper，在GC的流程上，没有太细节的说明，paper加上自己的理解，大致是这样：
1) 某节点生成checkpoint(snapshot)，他会将自己的位点n，以及checkpoint内容，签名，跑一次pbft共识。  
2）当checkpoint消息commit后，多数派节点已达成共识，所以这个checkpoint是stable checkpoint，在这个位点前的所有log都可以安全被删除。
3) 当新增节点（或者离线较久的节点）要从checkpoint拉数据时，需要比较proof。


## View Change

与raft一样，view change是由backups发起的。
1）view change: 当backup认为primary异常时(crash or 作恶)，backup会向其它节点广播view-change的消息。view change作将view number + 1，这与raft是一致的
2) new view: 当有节点时收到2f+1个view change消息时(包括他自己)，则进行new view操作，该节点广播new view message到其它节点。其它节点收到new view message后，进行校验，通过后进行PBFT的prepare流程，直到reply

View Change以及GC在工程实现上，应该远没有paper描述的那么简单，应该结合源码，或者自己实现一次，会更深入了解。

## 总结

本文首先描述了PBFT的一些关键点，为什么是3f+1, 2f+1以及f+1，这对了解PBFT比较关键，特别是对于有分布式算法基础的人。后续简单介绍了PBFT的三个主要流程。  

另外，本人还是认为用代码实现一次跟看N遍论文的效果是完全不同，本人计划后面有时间会用代码demo一遍，所以这里的太细节元素就略去了，待实现时再完全补全技术细节。  












